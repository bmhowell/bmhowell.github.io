<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>le brian howell | Brian Howell is currently a PhD candidate in the Multiphysics Simulation and Optimization Laboratory at UC Berkeley.</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="le brian howell" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Brian Howell is currently a PhD candidate in the Multiphysics Simulation and Optimization Laboratory at UC Berkeley." />
<meta property="og:description" content="Brian Howell is currently a PhD candidate in the Multiphysics Simulation and Optimization Laboratory at UC Berkeley." />
<link rel="canonical" href="http://localhost:4000/technicals.html" />
<meta property="og:url" content="http://localhost:4000/technicals.html" />
<meta property="og:site_name" content="le brian howell" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="le brian howell" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Brian Howell is currently a PhD candidate in the Multiphysics Simulation and Optimization Laboratory at UC Berkeley.","headline":"le brian howell","url":"http://localhost:4000/technicals.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="le brian howell" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">le brian howell</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <style>
    body {
      background-color: black;
      color: white;
    }
</style>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Technicals</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
        }

        .equation {
            color: white;
            font-size: 18px;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <h1>Technicals</h1>

    <h2>Multi-Layer Perceptron</h2>

    <figure>
        <img src="images/mlp.png" alt="Example Image">
        <figcaption>A typical visual representation of fully-connected 4-layer neural network or multi-layer perception (MLP) where \(\mathbf{x}_i\) is input vector to the next layer, \(\mathbf{W}_i\) are te matrix weights, and \(n_i\) are the number of neurons in a layer.</figcaption>
    </figure>

    Neural networks are a subset of machine learning algorithms that are designed to model the behavior of the human brain. They are composed of interconnected nodes or neurons, which are organized into layers and process information in a parallel and distributed manner. Neural networks have gained significant attention in recent years due to their ability to solve complex problems in various domains, including image and speech recognition, natural language processing, and predictive analytics. One of the key strengths of neural networks is their ability to approximate any function to arbitrary accuracy, given a sufficiently large number of neurons in the hidden layers. This property, known as universal approximation, has made neural networks an essential tool for data scientists and researchers seeking to develop intelligent systems capable of learning and adapting to changing environments. In this background section, we will provide a comprehensive overview of the fundamentals of neural networks, including their architecture, training algorithms, and applications.



Multi-layer perceptions (MLPs) are a class of feed-forward neural networks that consist of one or more hidden layers of fully connected neurons (see \Cref{fig:mlp}). In an MLP, each neuron receives inputs from all the neurons in the previous layer and computes a weighted sum of these inputs with additional bias term $b$. The weighted sum is then passed through an activation function, which introduces non-linearity into the model. The most commonly used activation functions include \texttt{sigmoid}, \texttt{ReLU}, and \texttt{tanh}. The output of each neuron is then passed on to the neurons in the next layer until the output layer is reached, which produces the final output of the network. This sequence of operations is typically referred to as the \textit{forward pass}. The neural network \textit{learns} by iteratively updating itself using optimization methods that minimizes the error between output of the neural network and the output from the training data, also known as the \textit{backward pass} or \textit{backward propagation}. Neural networks are a subset of machine learning algorithms that are designed to model the behavior of the human brain. They are composed of interconnected nodes or neurons, which are organized into layers and process information in a parallel and distributed manner. Neural networks have gained significant attention in recent years due to their ability to solve complex problems in various domains, including image and speech recognition, natural language processing, and predictive analytics. One of the key strengths of neural networks is their ability to approximate any function to arbitrary accuracy, given a sufficiently large number of neurons in the hidden layers. This property, known as universal approximation, has made neural networks an essential tool for data scientists and researchers seeking to develop intelligent systems capable of learning and adapting to changing environments. In this background section, we will provide a comprehensive overview of the fundamentals of neural networks, including their architecture, training algorithms, and applications.



Multi-layer perceptions (MLPs) are a class of feed-forward neural networks that consist of one or more hidden layers of fully connected neurons (see \Cref{fig:mlp}). In an MLP, each neuron receives inputs from all the neurons in the previous layer and computes a weighted sum of these inputs with additional bias term $b$. The weighted sum is then passed through an activation function, which introduces non-linearity into the model. The most commonly used activation functions include \texttt{sigmoid}, \texttt{ReLU}, and \texttt{tanh}. The output of each neuron is then passed on to the neurons in the next layer until the output layer is reached, which produces the final output of the network. This sequence of operations is typically referred to as the \textit{forward pass}. The neural network \textit{learns} by iteratively updating itself using optimization methods that minimizes the error between output of the neural network and the output from the training data, also known as the \textit{backward pass} or \textit{backward propagation}. 



    <div class="equation">
        Here is an example equation using LaTeX:
        \( \phi\left(\mathbf{W}_1 \mathbf{x}_0\right) = 
        \phi\left(
        \begin{bmatrix}
            W_{0,0}^{1} & W_{0,1}^{1} & W_{0,2}^{1} & W_{0,3}^{1} & b_0\\
            W_{1,0}^{1} & W_{1,1}^{1} & W_{1,2}^{1} & W_{1,3}^{1} & b_1\\
            W_{2,0}^{1} & W_{2,1}^{1} & W_{2,2}^{1} & W_{2,3}^{1} & b_2\\
            W_{3,0}^{1} & W_{3,1}^{1} & W_{3,2}^{1} & W_{3,3}^{1} & b_3\\
            W_{4,0}^{1} & W_{4,1}^{1} & W_{4,2}^{1} & W_{4,3}^{1} & b_4\\
            % W_{5,0}^{1} & W_{5,1}^{1} & W_{5,2}^{1} & W_{5,3}^{1} & b_5\\
        \end{bmatrix}
        \begin{bmatrix}
            x_0^{0} \\
            x_1^{0} \\
            x_2^{0} \\
            x_3^{0} \\
            1
        \end{bmatrix}
        \right)
        = \mathbf{x}_1 \).
    </div>
    

    <h2>Other Content</h2>
    <p>
        Your additional technical content goes here.
    </p>

    <script>
        MathJax.typeset();
    </script>
</body>
</html>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">le brian howell</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">le brian howell</li><li><a class="u-email" href="mailto:lebrianhowell@gmail.com">lebrianhowell@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/bmhowell"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">bmhowell</span></a></li><li><a href="https://www.twitter.com/__bhowell__"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">__bhowell__</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Brian Howell is currently a PhD candidate in the Multiphysics Simulation and  Optimization Laboratory at UC Berkeley. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
